{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GCP input data\n",
    "PROJECT_ID = 'intricate-reef-411403'\n",
    "BUCKET = \"test_bucket-intricate-reef-41103\"\n",
    "PROJECT_HOME = os.getenv(\"HOME\")+\"/data-engineering-zoomcamp/project\" # this may need to be updated when dockerized\n",
    "credentials_location = PROJECT_HOME+\"/.google/credentials/gcp.json\"\n",
    "\n",
    "# Get file structure data\n",
    "local_data_path = \"/.project/data/raw/Mendeley_data/\" # this will need to change when dockerized\n",
    "temp_path = \"/.project/data/raw/temp/\"\n",
    "local_data_file = \"100_Batches_IndPenSim_V3.csv\"\n",
    "path_to_local_home = os.environ.get(\"AIRFLOW_HOME\", \"/opt/airflow/\")\n",
    "gcs_input_path = \"raw/\"\n",
    "gcs_output_path = \"processed/raman_context/\"\n",
    "spark_jar_path = f\"{PROJECT_HOME}/lib/gcs-connector-hadoop3-2.2.5.jar,{PROJECT_HOME}/lib/spark-3.5-bigquery-0.37.0.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start spark standalone instance with worker\n",
    "start_spark_master = \"cd $SPARK_HOME && ./sbin/start-master.sh --port 7078\"\n",
    "start_spark_worker = \"cd $SPARK_HOME && ./sbin/start-worker.sh spark://127.0.0.1:7078\"\n",
    "\n",
    "start_master_process = subprocess.Popen(start_spark_master, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "start_master_output, start_master_error = start_master_process.communicate()\n",
    "\n",
    "start_worker_process = subprocess.Popen(start_spark_worker, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "start_worker_output, start_worker_error = start_worker_process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define spark configuration\n",
    "conf = SparkConf() \\\n",
    "    .setMaster(\"spark://127.0.0.1:7078\") \\\n",
    "    .setAppName(\"process_raw_data\") \\\n",
    "    .set(\"spark.jars\", spark_jar_path) \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 08:38:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# set up spark context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session using standalone cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Gather Data\n",
    "df_raw_values = spark.read.parquet('gs://test_bucket-intricate-reef-41103/raw/*.parquet')\n",
    "df_context = spark.read.parquet('gs://test_bucket-intricate-reef-41103/processed/sample_context/*.parquet')\n",
    "# filter columns\n",
    "raman_cols = [\"id\",\" 1-Raman spec recorded\",\"2-PAT control(PAT_ref:PAT ref)\",\"Fault flag\"]\n",
    "raman_cols.extend([str(i) for i in list(range(350,1751))])\n",
    "sample_cols = [\"id\",\"Time (h)\",\"Penicillin concentration(P:g/L)\",\"Fault reference(Fault_ref:Fault ref)\",\"0 - Recipe driven 1 - Operator controlled(Control_ref:Control ref)\"]\n",
    "# split raw df into relevant sample values and raman measurement data\n",
    "df_samples = df_raw_values.select(sample_cols) \\\n",
    "    .withColumnRenamed(\"id\",\"id_sample\")\n",
    "df_raman = df_raw_values.select(raman_cols) \\\n",
    "    .withColumnRenamed(\"id\",\"id_raman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# find most recent existing record in T_SAMPLE_CONTEXT\n",
    "# gather all sample records produced in the last 5 minutes\n",
    "#current_time = datetime.utcnow()\n",
    "current_time = \"2024-04-14 02:55:00\"\n",
    "current_time = datetime.strptime(current_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "back_time = current_time - timedelta(minutes=5)\n",
    "current_ts = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "back_ts = back_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "date_range = (current_ts,back_ts)\n",
    "where_clause = \"\"\"sample_ts BETWEEN TO_TIMESTAMP('{1}','yyyy-MM-dd HH:mm:ss') AND TO_TIMESTAMP('{0}','yyyy-MM-dd HH:mm:ss')\"\"\".format(*date_range)\n",
    "try:\n",
    "    most_recent_time = spark.read.format(\"bigquery\") \\\n",
    "            .option(\"project\",PROJECT_ID) \\\n",
    "            .option(\"dataset\",\"test_schema\") \\\n",
    "            .option(\"table\",\"test_table\") \\\n",
    "            .load() \\\n",
    "            .where(where_clause) \\\n",
    "            .agg(F.max(\"sample_ts\")) \\\n",
    "            .collect()[0][0]\n",
    "except Exception as e:\n",
    "    print(f\"Error - no existing records found: {e}\")\n",
    "    most_recent_time = back_time\n",
    "most_recent_ts = most_recent_time.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather new data that has not been traced into T_SAMPLE_CONTEXT\n",
    "date_range = (current_ts,most_recent_ts)\n",
    "where_clause = \"\"\"sample_ts BETWEEN TO_TIMESTAMP('{1}','yyyy-MM-dd HH:mm:ss') AND TO_TIMESTAMP('{0}','yyyy-MM-dd HH:mm:ss')\"\"\".format(*date_range)\n",
    "df_context = df_context.select([\"id\",\"Batch Number\",\"sample_ts\"]) \\\n",
    "    .where(where_clause) \\\n",
    "    .withColumnRenamed(\"Batch Number\",\"batch_number\")\n",
    "# join to raman and sample dfs\n",
    "df_sample_context = df_samples.join(df_context,df_samples.id_sample == df_context.id,\"inner\").drop(\"id_sample\")\n",
    "df_raman_context = df_raman.join(df_context,df_raman.id_raman == df_context.id,\"inner\").drop(\"id_raman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "sample_colnames = [\"time_hrs\",\"penicillin_concentration_g_l\",\"fault_reference\",\"recipe_0_or_operator_1_controlled\",\"id\",\"batch_number\",\"sample_ts\"]\n",
    "raman_colnames = [\"1_raman_spec_recorded\",\"2_pat_control\",\"fault_flag\"]\n",
    "raman_colnames.extend([str(n) for n in range(350,1751)])\n",
    "raman_colnames.extend([\"id\",\"batch_number\",\"sample_ts\"])\n",
    "df_sample_context = df_sample_context.toDF(*sample_colnames)\n",
    "df_raman_context = df_raman_context.toDF(*raman_colnames)\n",
    "# fill null values with 0\n",
    "df_sample_context = df_sample_context.fillna(0)\n",
    "df_raman_context = df_raman_context.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema for T_SAMPLE_CONTEXT\n",
    "sample_schema = T.StructType([\n",
    "    T.StructField(\"time_hrs\",T.DoubleType()),\n",
    "    T.StructField(\"penicillin_concentration_g_l\",T.DoubleType()),\n",
    "    T.StructField(\"fault_reference\",T.LongType()),\n",
    "    T.StructField(\"recipe_0_or_operator_1_controlled\",T.LongType()),\n",
    "    T.StructField(\"id\",T.IntegerType()),\n",
    "    T.StructField(\"batch_number\",T.LongType()),\n",
    "    T.StructField(\"sample_ts\",T.TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema for T_RAMAN_CONTEXT\n",
    "raman_col_types = [\n",
    "    T.StructField(\"id\", T.IntegerType()),\n",
    "    T.StructField(\"batch_number\",T.LongType()),\n",
    "    T.StructField(\"sample_ts\",T.TimestampType()),\n",
    "    T.StructField(\"1_raman_spec_recorded\",T.LongType()),\n",
    "    T.StructField(\"2_pat_control\",T.LongType()),\n",
    "    T.StructField(\"fault_flag\",T.LongType())\n",
    "]\n",
    "wavelengths = []\n",
    "for i in range(350,1751):\n",
    "    wavelength = T.StructField(str(i),T.DoubleType())\n",
    "    wavelengths.append(wavelength)\n",
    "raman_col_types.extend(wavelengths)\n",
    "raman_schema = T.StructType(raman_col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new sample context data to table\n",
    "df_sample_context.write.format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\", BUCKET) \\\n",
    "    .option(\"table\", PROJECT_ID+\".test_schema.t_sample_context\") \\\n",
    "    .option(\"createDisposition\", \"CREATE_IF_NEEDED\") \\\n",
    "    .option(\"writeDisposition\", \"WRITE_TRUNCATE\") \\\n",
    "    .option(\"schema\", sample_schema.json()) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 08:39:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/04/14 08:40:24 WARN TaskSetManager: Lost task 2.0 in stage 6.0 (TID 10) (10.168.0.7 executor 0): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64)\n",
      "\tat org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71)\n",
      "\tat org.apache.parquet.hadoop.codec.NonBlockedDecompressorStream.<init>(NonBlockedDecompressorStream.java:36)\n",
      "\tat org.apache.parquet.hadoop.codec.SnappyCodec.createInputStream(SnappyCodec.java:75)\n",
      "\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesDecompressor.decompress(CodecFactory.java:112)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader.readDictionaryPage(ColumnChunkPageReadStore.java:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.<init>(VectorizedColumnReader.java:120)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initColumnReader(VectorizedParquetRecordReader.java:437)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:427)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:335)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$1408/0x0000000100f6b840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1254/0x0000000100e0f040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\n",
      "24/04/14 08:40:25 ERROR TaskSchedulerImpl: Lost executor 0 on 10.168.0.7: Command exited with code 52\n",
      "24/04/14 08:40:25 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 8) (10.168.0.7 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "24/04/14 08:40:25 WARN TaskSetManager: Lost task 3.0 in stage 6.0 (TID 11) (10.168.0.7 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "24/04/14 08:40:25 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 9) (10.168.0.7 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "24/04/14 08:40:25 WARN TaskSetManager: Lost task 4.0 in stage 6.0 (TID 12) (10.168.0.7 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "[Stage 6:>                                                         (0 + 4) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-04-14T08:40:47.622+0000\u001b[0m] {\u001b[34mjava_gateway.py:\u001b[0m1066} ERROR\u001b[0m - KeyboardInterrupt while sending command.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jdelzio/data-engineering-zoomcamp/project/.venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jdelzio/data-engineering-zoomcamp/project/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[\u001b[34m2024-04-14T08:40:47.630+0000\u001b[0m] {\u001b[34mclientserver.py:\u001b[0m543} INFO\u001b[0m - Closing down clientserver connection\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# raman dataset is very large, save to GCS Bucket with partitions\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df_raman_repartitioned \u001b[38;5;241m=\u001b[39m df_raman_context\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf_raman_repartitioned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBUCKET\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgcs_output_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-engineering-zoomcamp/project/.venv/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-engineering-zoomcamp/project/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/data-engineering-zoomcamp/project/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/data-engineering-zoomcamp/project/.venv/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 08:40:59 WARN TaskSetManager: Lost task 3.1 in stage 6.0 (TID 15) (10.168.0.7 executor 1): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$720/0x0000000100794840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$721/0x0000000100795040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$570/0x0000000100642440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\n",
      "24/04/14 08:40:59 ERROR TaskSchedulerImpl: Lost executor 1 on 10.168.0.7: Command exited with code 52\n",
      "24/04/14 08:40:59 WARN TaskSetManager: Lost task 2.1 in stage 6.0 (TID 17) (10.168.0.7 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "24/04/14 08:40:59 WARN TaskSetManager: Lost task 1.1 in stage 6.0 (TID 14) (10.168.0.7 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "24/04/14 08:40:59 WARN TaskSetManager: Lost task 4.1 in stage 6.0 (TID 13) (10.168.0.7 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "24/04/14 08:40:59 WARN TaskSetManager: Lost task 0.1 in stage 6.0 (TID 16) (10.168.0.7 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "[Stage 6:>                                                         (0 + 4) / 13]\r"
     ]
    }
   ],
   "source": [
    "# add new sample context data to table\n",
    "df_raman_context.write.format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\", BUCKET) \\\n",
    "    .option(\"table\", PROJECT_ID+\".test_schema.t_raman_context\") \\\n",
    "    .option(\"createDisposition\", \"CREATE_IF_NEEDED\") \\\n",
    "    .option(\"writeDisposition\", \"WRITE_TRUNCATE\") \\\n",
    "    .option(\"schema\", raman_schema.json()) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    }
   ],
   "source": [
    "# Stop Local Standalone cluster\n",
    "!cd $SPARK_HOME && ./sbin/stop-master.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "stopping org.apache.spark.deploy.worker.Worker\n"
     ]
    }
   ],
   "source": [
    "# Stop Worker\n",
    "!cd $SPARK_HOME && ./sbin/stop-worker.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
