{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar lib/gcs-connector-hadoop3-2.2.5.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify spark environment template to fix host name for dockerization - ONLY NEED TO RUN ONCE\n",
    "!cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh && echo \"SPARK_MASTER_HOST=127.0.0.1\" >> $SPARK_HOME/conf/spark-env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /home/jdelzio/spark/spark-3.5.1-bin-hadoop3/logs/spark-jdelzio-org.apache.spark.deploy.master.Master-1-de-zoomcamp-deb.us-west2-a.c.intricate-reef-411403.internal.out\n"
     ]
    }
   ],
   "source": [
    "# Start Local Standalone cluster\n",
    "!cd $SPARK_HOME && ./sbin/start-master.sh --host 127.0.0.1 --port 7078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.worker.Worker, logging to /home/jdelzio/spark/spark-3.5.1-bin-hadoop3/logs/spark-jdelzio-org.apache.spark.deploy.worker.Worker-1-de-zoomcamp-deb.us-west2-a.c.intricate-reef-411403.internal.out\n"
     ]
    }
   ],
   "source": [
    "# Start cluster worker\n",
    "!cd $SPARK_HOME && ./sbin/start-worker.sh spark://127.0.0.1:7078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up spark configuration to GCP\n",
    "PROJECT_HOME = os.getenv(\"HOME\")+\"/data-engineering-zoomcamp/project\" # this may need to be updated when dockerized\n",
    "credentials_location = PROJECT_HOME+\"/.google/credentials/gcp.json\"\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster(\"spark://127.0.0.1:7078\") \\\n",
    "    .setAppName(\"process_raw_data\") \\\n",
    "    .set(\"spark.jars\", PROJECT_HOME+\"/lib/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/13 22:50:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# set up spark context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session using standalone cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pull data from GCS Bucket into spark df\n",
    "#gs://test_bucket-intricate-reef-41103/raw/*\n",
    "\n",
    "df_test = spark.read.parquet('gs://test_bucket-intricate-reef-41103/raw/*') \\\n",
    "    .select([\"id\",\"Time (h)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|Time (h)|\n",
      "+---+--------+\n",
      "|  1|     0.2|\n",
      "|  2|     0.4|\n",
      "|  3|     0.6|\n",
      "|  4|     0.8|\n",
      "|  5|     1.0|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values with 0\n",
    "#df_filled = df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by id, save row count\n",
    "df_sorted = df_test.orderBy(\"id\")\n",
    "nrows = df_sorted.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find new batch start indeces\n",
    "batch_start_df = df_sorted \\\n",
    "    .filter(df_sorted[\"Time (h)\"] == 0.2) \\\n",
    "    .select(\"id\") \\\n",
    "    .withColumnRenamed(\"id\",\"batch_start_id\") \\\n",
    "    .withColumn(\"Batch Number\",F.monotonically_increasing_id()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add next back id for join clause\n",
    "window_frame = Window.orderBy(\"batch_start_id\")\n",
    "batch_start_df = batch_start_df.withColumn(\"next_batch_start_id\", F.lead(\"batch_start_id\").over(window_frame))\n",
    "# fill final next_batch_start_id with nrow df_sorted + 1\n",
    "batch_start_df = batch_start_df.fillna(nrows+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join batch numbers to df_sorted\n",
    "df_processed = df_sorted.join(batch_start_df, (df_sorted.id >= batch_start_df.batch_start_id) & (df_sorted.id < batch_start_df.next_batch_start_id ), \"inner\")\n",
    "df_processed = df_processed.drop(*[\"batch_start_id\",\"next_batch_start_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.collect()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to simulate that 30 batches worth of the dataset have already been completed, while the final 70 are still to be peformed\n",
    "completed_batches = 30\n",
    "first_new_batch = df_processed \\\n",
    "    .filter(df_processed[\"Batch Number\"] == completed_batches+1) \\\n",
    "    .select(\"id\") \\\n",
    "    .head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate artificial sample production timestamps at around 0.06s per sample (of course this is highly accelerated for quick demonstration purposes)\n",
    "# The final sample will be consumed a little over an hour from the current time\n",
    "ts_current = datetime.utcnow()\n",
    "ts_first_30_batches = [ts_current - i*timedelta(seconds=0.06) for i in range(1,first_new_batch)]\n",
    "ts_first_30_batches.reverse()\n",
    "ts_last_70_batches = [ts_current + i*timedelta(seconds=0.06) for i in range(first_new_batch,nrows+1)]\n",
    "sample_ts = ts_first_30_batches\n",
    "sample_ts.extend(ts_last_70_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_ts) == nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join sample ts to processed_df\n",
    "sample_ts_df = spark.createDataFrame([Row(index=i+1, sample_ts=sample_ts[i]) for i in range(nrows)])\n",
    "df_processed = df_processed.join(sample_ts_df, df_processed.id == sample_ts_df.index, \"inner\").drop(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed \\\n",
    "    .repartition(4) \\\n",
    "    .write.parquet('gs://test_bucket-intricate-reef-41103/processed/sample_context/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    }
   ],
   "source": [
    "# Stop Local Standalone cluster\n",
    "!cd $SPARK_HOME && ./sbin/stop-master.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping org.apache.spark.deploy.worker.Worker\n"
     ]
    }
   ],
   "source": [
    "# Stop Worker\n",
    "!cd $SPARK_HOME && ./sbin/stop-worker.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
